diff --git a/synk/pool.go b/synk/pool.go
index 75f057d..c7fc6e6 100644
--- a/synk/pool.go
+++ b/synk/pool.go
@@ -11,14 +11,14 @@ import (
 )
 
 type UnsizedBytesPool struct {
-	pool chan weakBuf
+	pool chan strongBuf
 }
 
 type SizedBytesPool struct {
 	// 1024*(2<<i) bytes
 	// 4KiB, 8KiB, 16KiB, 32KiB, 64KiB
 	// , 128KiB, 256KiB, 512KiB, 1MiB, 2MiB, 4MiB
-	pools     [SizedPools]chan weakBuf
+	pools     [SizedPools]chan strongBuf
 	smallPool chan weakBuf // everything smaller than allocSize(0)
 	largePool chan weakBuf // everything larger than allocSize(SizedPools-1)
 
@@ -74,7 +74,7 @@ func init() {
 func initAll() {
 	sizedFullCaps = xsync.NewMap[*byte, int]()
 
-	unsizedBytesPool.pool = make(chan weakBuf, UnsizedPoolSize)
+	unsizedBytesPool.pool = make(chan strongBuf, UnsizedPoolSize)
 
 	for i := range allocSizes {
 		allocSizes[i] = 1024 * (2 << i)
@@ -85,7 +85,7 @@ func initAll() {
 	sizedBytesPool.smallPool = make(chan weakBuf, SmallPoolChannelSize)
 	sizedBytesPool.largePool = make(chan weakBuf, LargePoolChannelSize)
 	for i := range sizedBytesPool.pools {
-		sizedBytesPool.pools[i] = make(chan weakBuf, poolChannelSize(i))
+		sizedBytesPool.pools[i] = make(chan strongBuf, poolChannelSize(i))
 	}
 
 	// Initialize sync pool version
@@ -124,12 +124,9 @@ func (p *SizedBytesPool) PutBuffer(buf *bytes.Buffer) {
 func (p UnsizedBytesPool) Get() []byte {
 	for {
 		select {
-		case bWeak := <-p.pool:
-			b := getBufFromWeak(bWeak)
-			if b == nil {
-				continue
-			}
-			addReused(cap(b))
+		case bStrong := <-p.pool:
+			b := getBufFromStrong(bStrong)
+			addReused(bStrong.cap)
 			return b[:0]
 		default:
 			addNonPooled(MinAllocSize)
@@ -154,8 +151,8 @@ func (p *SizedBytesPool) GetSized(size int) []byte {
 	idx := targetIdx
 	for idx < SizedPools {
 		select {
-		case bWeak := <-p.pools[idx]:
-			b := getBufFromWeak(bWeak)
+		case bStrong := <-p.pools[idx]:
+			b := getBufFromStrong(bStrong)
 			if b == nil {
 				continue // try same pool again
 			}
@@ -188,7 +185,7 @@ func (p *SizedBytesPool) GetSized(size int) []byte {
 
 //go:inline
 func (p UnsizedBytesPool) Put(b []byte) {
-	put(b, p.pool)
+	putStrong(b, p.pool)
 }
 
 func (p *SizedBytesPool) Put(b []byte) {
@@ -214,7 +211,7 @@ func (p *SizedBytesPool) put(b []byte, isRemaining bool) {
 		if capB < allocSize(idx) {
 			idx--
 		}
-		put(b, p.pools[idx])
+		putStrong(b, p.pools[idx])
 		if isRemaining {
 			addReusedRemaining(capB)
 		}
@@ -248,6 +245,27 @@ func pullOrGrow(pool chan weakBuf, size int) []byte {
 	}
 }
 
+func pullOrGrowStrong(pool chan strongBuf, size int) []byte {
+	for {
+		select {
+		case bStrong := <-pool:
+			b := getBufFromStrong(bStrong)
+			capB := cap(b)
+			if capB < size {
+				addDropped(size - capB)
+				addNonPooled(size - capB)
+				newB := slices.Grow(b, size)
+				return newB[:size]
+			}
+			addReused(capB)
+			return b[:size]
+		default:
+			addNonPooled(size)
+			return make([]byte, size)
+		}
+	}
+}
+
 //go:inline
 func put(b []byte, pool chan weakBuf) {
 	w := makeWeak(b)
@@ -260,11 +278,27 @@ func put(b []byte, pool chan weakBuf) {
 	}
 }
 
+func putStrong(b []byte, pool chan strongBuf) {
+	s := makeStrong(b)
+
+	select {
+	case pool <- s:
+	default:
+		addDropped(s.cap)
+		// just drop it
+	}
+}
+
 type weakBuf struct {
 	ptr weak.Pointer[byte]
 	cap int
 }
 
+type strongBuf struct {
+	ptr *byte
+	cap int
+}
+
 //go:inline
 func makeWeak(b []byte) weakBuf {
 	return weakBuf{
@@ -273,6 +307,14 @@ func makeWeak(b []byte) weakBuf {
 	}
 }
 
+//go:inline
+func makeStrong(b []byte) strongBuf {
+	return strongBuf{
+		ptr: unsafe.SliceData(b),
+		cap: cap(b),
+	}
+}
+
 //go:inline
 func getBufFromWeak(w weakBuf) []byte {
 	ptr := w.ptr.Value()
@@ -286,6 +328,10 @@ func getBufFromWeak(w weakBuf) []byte {
 	return nil
 }
 
+func getBufFromStrong(b strongBuf) []byte {
+	return unsafe.Slice(b.ptr, b.cap)
+}
+
 // it should be used for sized bytes pool only,
 // since unsized bytes can grow and causes entries leaked in sizedFullCaps
 func storeFullCap(b []byte, c int) {
